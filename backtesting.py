# Preprocessing
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, confusion_matrix, classification_report
from utils import status_calc, status_calc_self
from pprint import pprint
import csv

def backtest():
    """
    A simple backtest, which splits the dataset into a train set and test set,
    then fits a Random Forest classifier to the train set. We print the precision and accuracy
    of the classifier on the test set, then run a backtest comparing this strategy's performance
    to passive investment in the S&P500.
    Please note that there is a methodological flaw in this backtest which will give deceptively
    good results, so the results here should not encourage you to live trade.
    """
    # Build the dataset, and drop any rows with missing values
    # data_df = pd.read_csv("keystats.csv", index_col="Date")
    data_df = pd.read_csv("keystats.csv", index_col="Date")
    data_df.drop([
                # "Market Cap",
                "Enterprise Value",
                # "Trailing P/E",
                # "Forward P/E",
                # "PEG Ratio",
                # "Price/Sales",
                # "Price/Book",
                # "Enterprise Value/Revenue",
                # "Enterprise Value/EBITDA",
                # "Profit Margin",
                # "Operating Margin",
                # "Return on Assets",
                # "Return on Equity",
                # "Revenue",
                # "Revenue Per Share",
                "Qtrly Revenue Growth",
                # "Gross Profit",
                # "EBITDA",
                "Net Income Avl to Common",
                # "Diluted EPS",
                "Qtrly Earnings Growth",
                # "Total Cash",
                # "Total Cash Per Share",
                "Total Debt",
                # "Total Debt/Equity",
                # "Current Ratio",
                # "Book Value Per Share",
                "Operating Cash Flow",
                "Levered Free Cash Flow",
                # "Beta",
                # "50-Day Moving Average",
                # "200-Day Moving Average",
                # "Avg Vol (3 month)",
                # "Shares Outstanding",
                # "Float",
                # "% Held by Insiders",
                # "% Held by Institutions",
                # "Shares Short (as of",
                # "Short Ratio",
                # "Short % of Float",
                # "Shares Short (prior month)",
                ], axis=1,inplace=True)
    data_df.dropna(axis=0, how="any", inplace=True)
    # print(data_df)
    features = data_df.columns[6:]
    X = data_df[features].values

    # The labels are generated by applying the status_calc to the dataframe.
    # '1' if a stock beats the S&P500 by more than x%, else '0'. Here x is the
    # outperformance parameter, which is set to 10 by default but can be redefined.
    # y = list(
    #     status_calc(
    #         data_df["stock_p_change"], data_df["SP500_p_change"], outperformance=10
    #         )
    #     )

    y = list(
       status_calc_self(
           data_df["stock_p_change"], outperformance=15
           )
       )
    ticker = data_df["Ticker"]
    # print(ticker)

    # z is required for us to track returns
    z = np.array(data_df[["stock_p_change", "SP500_p_change"]])

    # Generate the train set and test set by randomly splitting the dataset
    X_train, X_test, y_train, y_test, z_train, z_test, ticker_train, ticker_test = train_test_split(
        X, y, z, ticker, test_size=0.2
    )



    # Look at parameters used by our current forest
    # print('Parameters currently in use:\n')
    # pprint(clf.get_params())


    # # Number of trees in random forest
    # n_estimators = [int(x) for x in np.linspace(start = 200, stop = 700, num = 10)]
    # # Number of features to consider at every split
    # max_features = ['sqrt','log2']
    # # Maximum number of levels in tree
    # max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
    # max_depth.append(None)
    # # Minimum number of samples required to split a node
    # min_samples_split = [3, 5, 7]
    # # Minimum number of samples required at each leaf node
    # min_samples_leaf = [1]
    # # Method of selecting samples for training each tree
    # bootstrap = [True, False]
    # # Create the random grid
    # random_grid = {'n_estimators': n_estimators,
    #                'max_features': max_features,
    #                'max_depth': max_depth,
    #                'min_samples_split': min_samples_split,
    #                'min_samples_leaf': min_samples_leaf,
    #                'bootstrap': bootstrap}
    # pprint(random_grid)

    # clf = RandomForestClassifier()
    # clf_grid_search = GridSearchCV(
    #                         estimator = clf, 
    #                         param_grid = random_grid,  
    #                         cv = 3, 
    #                         verbose=2, 
    #                         n_jobs = -1
    #                         )
    # clf_grid_search.fit(X_train, y_train)
    # pprint(clf_grid_search.best_params_)
    
    # best_grid = clf_grid_search.best_estimator_
    # grid_accuracy = evaluate(best_grid, X_train, y_train)

    #Instantiate a RandomForestClassifier with 100 trees, then fit it to the training data
    clf = RandomForestClassifier(
                                bootstrap= False,
                                max_depth=None,
                                max_features= 'sqrt',
                                min_samples_leaf= 1,
                                min_samples_split= 3,
                                n_estimators= 200
                                )
    clf.fit(X_train, y_train)

    # Generate the predictions, then print test set accuracy and precision
    y_pred = clf.predict(X_test)
    print("Classifier performance\n", "=" * 20)
    # print(f"Accuracy score: {clf.score(X_test, y_test): .2f}")
    # print(f"Precision score: {precision_score(y_test, y_pred): .2f}")

    matrix = confusion_matrix(y_test, y_pred)
    print(matrix)

    print(classification_report(y_test, y_pred))
    # Because y_pred is an array of 1s and 0s, the number of positive predictions
    # is equal to the sum of the array
    num_positive_predictions = sum(y_pred)
    if num_positive_predictions < 0:
        print("No stocks predicted!")


    # ticker_pred = ticker_test[y_pred]
    # print(y_pred) #Tyler
    # print(y_test)

    # Recall that z_test stores the change in stock price in column 0, and the
    # change in S&P500 price in column 1.
    # Whenever a stock is predicted to outperform (y_pred = 1), we 'buy' that stock
    # and simultaneously `buy` the index for comparison.
    stock_returns = z_test[y_pred, 0] / 100
    market_returns = z_test[y_pred, 1] / 100

    # Calculate the average growth for each stock we predicted 'buy'
    # and the corresponding index growth
    avg_predicted_stock_growth = sum(stock_returns) / num_positive_predictions
    index_growth = sum(market_returns) / num_positive_predictions
    percentage_stock_returns = 100 * (avg_predicted_stock_growth)
    percentage_market_returns = 100 * (index_growth)
    total_outperformance = percentage_stock_returns - percentage_market_returns

    print("\n Stock prediction performance report \n", "=" * 40)
    print(f"Total Trades:", num_positive_predictions)
    print(f"Average return for stock predictions: {percentage_stock_returns: .1f} %")
    print(
        f"Average market return in the same period: {percentage_market_returns: .1f}% "
    )
    print(
        f"Compared to the index, our strategy earns {total_outperformance: .1f} percentage points more"
    )

    features_list = [
    "Market Cap",
    # "Enterprise Value",
    "Trailing P/E",
    "Forward P/E",
    "PEG Ratio",
    "Price/Sales",
    "Price/Book",
    "Enterprise Value/Revenue",
    "Enterprise Value/EBITDA",
    "Profit Margin",
    "Operating Margin",
    "Return on Assets",
    "Return on Equity",
    "Revenue",
    "Revenue Per Share",
    # "Qtrly Revenue Growth",
    "Gross Profit",
    "EBITDA",
    # "Net Income Avl to Common",
    "Diluted EPS",
    # "Qtrly Earnings Growth",
    "Total Cash",
    "Total Cash Per Share",
    # "Total Debt",
    "Total Debt/Equity",
    "Current Ratio",
    "Book Value Per Share",
    # "Operating Cash Flow",
    # "Levered Free Cash Flow",
    "Beta",
    "50-Day Moving Average",
    "200-Day Moving Average",
    "Avg Vol (3 month)",
    "Shares Outstanding",
    "Float",
    "% Held by Insiders",
    "% Held by Institutions",
    "Shares Short (as of",
    "Short Ratio",
    "Short % of Float",
    "Shares Short (prior month)",
    ]
    # Get numerical feature importances
    importances = list(clf.feature_importances_)
    # List of tuples with variable and importance
    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features_list, importances)]
    # Sort the feature importances by most important first
    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
    # Print out the feature and importances 
    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]
 



if __name__ == "__main__":
    backtest()
    
